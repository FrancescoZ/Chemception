import network
import input
import helpers
import Optimizer

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
import os
import sys

import numpy as nu

from sklearn.model_selection import train_test_split

seed = 7
nu.random.seed(seed)

N=16
inputSize = 80

if len(sys.argv)>1 and sys.argv[1]!=None:
	N=sys.argv[1]
	if len(sys.argv)>2 and sys.argv[2]!=None:
		inputSize=sys.argv[2]


batch_size 			= 32
num_classes 		= 2
epochs 				= 1
data_augmentation 	= False
num_predictions 	= 20
save_dir 			= os.path.join(os.getcwd(), 'saved_models')
learning_rate		= 1e-3
rho					= 0.9
epsilon				= 1e-8
cross_val			= 5

# The data, split between train and test sets:
(X, Y) 	= input.LoadInput(extensionImg='png',size=inputSize)

cvscores = []
for i in range(1,cross_val):

	model_name 			= 'chemception_trained_cross_'+str(i)+'_model.h5'
	X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1*i, random_state=seed)
	# create model	
	cross_val = cross_val +1	
	x_train = X_train
	y_train = Y_train
	print('%i %i %s %s',len(X_test),len(Y_test),X_train[0],Y_train[0])
	# Convert class vectors to binary class matrices.
	y_train 			= keras.utils.to_categorical(y_train, num_classes)
	Y_test 			= keras.utils.to_categorical(Y_test, num_classes)
	model 				= network.Chemception(N,inputSize)
	x_train 		= x_train.astype('float32')
	X_test 		= X_test.astype('float32')
	x_train 		/= 255
	X_test 			/= 255
	print('x_train shape:', x_train.shape)
	print(x_train.shape[0], 'train samples')

	# initiate RMSprop optimizer
	opt 				= keras.optimizers.RMSprop(lr=learning_rate, rho=rho, epsilon=epsilon, decay=0.0)

	# Let's train the model using RMSprop
	model.compile(loss='mean_squared_error',
				optimizer=opt,
				metrics=['accuracy'])
	learning_rate_init	= 1e-3
	momentum			= 0.9
	gamma				= 0.92
	sgd = SGD(lr=learning_rate_init, decay=0, momentum=momentum, nesterov=True)
	optCallback = Optimizer.OptimizerTracker()
	tensorBoard = keras.callbacks.TensorBoard(log_dir='./logs', 
			histogram_freq=0, 
			batch_size=batch_size, 
			write_graph=True, 
			write_grads=False, 
			write_images=False, 
			embeddings_freq=0, 
			embeddings_layer_names=None, 
			embeddings_metadata=None)

	if not data_augmentation:
		print('Not using data augmentation.')
		datagen = ImageDataGenerator(
			featurewise_center=False,  # set input mean to 0 over the dataset
			samplewise_center=False,  # set each sample mean to 0
			featurewise_std_normalization=False,  # divide inputs by std of the dataset
			samplewise_std_normalization=False,  # divide each input by its std
			zca_whitening=False,  # apply ZCA whitening
			rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
			width_shift_range=0,  # randomly shift images horizontally (fraction of total width)
			height_shift_range=0,  # randomly shift images vertically (fraction of total height)
			horizontal_flip=False,  # randomly flip images
			vertical_flip=False)  # randomly flip images

		# Compute quantities required for feature-wise normalization
		# (std, mean, and principal components if ZCA whitening is applied).
		datagen.fit(x_train)
		# Fit the model on the batches generated by datagen.flow().
		model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard])
		model.fit_generator(datagen.flow(x_train, y_train,
			batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard, optCallback])
	else:
		print('Using real-time data augmentation.')
		# This will do preprocessing and realtime data augmentation:
		datagen = ImageDataGenerator(
			featurewise_center=False,  # set input mean to 0 over the dataset
			samplewise_center=False,  # set each sample mean to 0
			featurewise_std_normalization=False,  # divide inputs by std of the dataset
			samplewise_std_normalization=False,  # divide each input by its std
			zca_whitening=False,  # apply ZCA whitening
			rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
			width_shift_range=0,  # randomly shift images horizontally (fraction of total width)
			height_shift_range=0,  # randomly shift images vertically (fraction of total height)
			horizontal_flip=True,  # randomly flip images
			vertical_flip=True)  # randomly flip images

		# Compute quantities required for feature-wise normalization
		# (std, mean, and principal components if ZCA whitening is applied).
		datagen.fit(x_train)
		# Fit the model on the batches generated by datagen.flow().
		model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard])
		model.fit_generator(datagen.flow(x_train, y_train,
			batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard, optCallback])
	# Save model and weights
	if not os.path.isdir(save_dir):
		os.makedirs(save_dir)
	model_path = os.path.join(save_dir, model_name)
	model.save(model_path)
	print('Saved trained model at %s ' % model_path)

	# Score trained model.
	scores = model.evaluate(X_test, Y_test, verbose=1)
	print('Test loss:', scores[0])
	print('Test accuracy:', scores[1])
	print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
	cvscores.append(scores[1] * 100)
print("%.2f%% (+/- %.2f%%)" % (nu.mean(cvscores), nu.std(cvscores)))