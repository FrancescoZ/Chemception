import network
import input as data
import helpers
import Optimizer
from evaluation import Metrics

import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from keras.callbacks import TensorBoard

import os
import sys
import time
import statistics
import shutil

import numpy as nu

from sklearn.model_selection import train_test_split

#Setting seed to re run the same simulation with the same result
seed = 7
nu.random.seed(seed)

#Defining the size of the network, this can be passed as parameter
N=16
inputSize = 80
#Execution name, if non will throw an error
if len(sys.argv)>1 and sys.argv[1]!=None:
	if os.path.isdir(sys.argv[1]):
		over = input('Execution folder already exist, to you want to overwrite it? [Y/N]')
		if str(over) == 'Y' or str(over)=='y':
			executionName = sys.argv[1]
			shutil.rmtree('./'+executionName, ignore_errors=True)
		else:
			raise AttributeError("Execution folder already exists")
	executionName = sys.argv[1]
	os.makedirs('./'+executionName)
else: 
	raise AttributeError("Execution name is missing")
#get the size of the simulation if given
if len(sys.argv)>2 and sys.argv[2]!=None:
	N=sys.argv[2]
	if len(sys.argv)>3 and sys.argv[3]!=None:
		inputSize=sys.argv[3]

#Setting of the network
batch_size 			= 32
num_classes 		= 2
epochs 				= 100
data_augmentation 	= False
learning_rate		= 1e-3
rho					= 0.9
epsilon				= 1e-8
cross_val			= 5
main_execution_path = './'+executionName+'/'
final_resume 		= main_execution_path + executionName + '_resume.txt'
# The data, split between train and test sets:
(X, Y) 	= data.LoadInput(extensionImg='png',size=inputSize,duplicateProb=1.e-2,seed=seed)

cvscores = []
for i in range(1,cross_val+1):

	
	model_name 						 = 'chemception_trained_cross_'+str(i)
	current_path 					 = './'+executionName+'/'+model_name
	os.makedirs(current_path)
	model_name_file 				 = model_name + '_model.h5'
	model_directory					 = current_path+'/model'
	os.makedirs(model_directory)
	model_path 						 = os.path.join(model_directory, model_name_file)
	log_dir							 = current_path+'/logs'
	resume_file						 = current_path + '/'+model_name+'_resume.txt'

	X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1*i, random_state=seed)
	# create model	
	cross_val 						 = cross_val +1	
	x_train 						 = X_train
	y_train 						 = Y_train
	
	# Convert class vectors to binary class matrices.
	y_train 			= keras.utils.to_categorical(y_train, num_classes)
	Y_test 				= keras.utils.to_categorical(Y_test, num_classes)
	model 				= network.Chemception(N,inputSize)
	x_train 			= x_train.astype('float32')
	X_test 				= X_test.astype('float32')
	x_train 			/= 255
	X_test 				/= 255
	print('x_train shape:', x_train.shape)
	print(x_train.shape[0], 'train samples')

	# initiate RMSprop optimizer
	opt 				= keras.optimizers.RMSprop(lr=learning_rate, rho=rho, epsilon=epsilon, decay=0.0)

	# Let's train the model using RMSprop
	model.compile(loss='mean_squared_error',
				optimizer=opt,
				metrics=['accuracy'])
	learning_rate_init	= 1e-3
	momentum			= 0.9
	gamma				= 0.92
	sgd = SGD(lr=learning_rate_init, decay=0, momentum=momentum, nesterov=True)
	optCallback = Optimizer.OptimizerTracker()
	tensorBoard = TensorBoard(log_dir=log_dir, 
			histogram_freq=1, 
			batch_size=batch_size, 
			write_graph=False, 
			write_grads=True, 
			write_images=True, 
			embeddings_freq=0, 
			embeddings_layer_names=None, 
			embeddings_metadata=None)
	metrics = Metrics()

	if not data_augmentation:
		print('Not using data augmentation.')
		datagen = ImageDataGenerator(
			featurewise_center=False,  # set input mean to 0 over the dataset
			samplewise_center=False,  # set each sample mean to 0
			featurewise_std_normalization=False,  # divide inputs by std of the dataset
			samplewise_std_normalization=False,  # divide each input by its std
			zca_whitening=False,  # apply ZCA whitening
			rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
			width_shift_range=0,  # randomly shift images horizontally (fraction of total width)
			height_shift_range=0,  # randomly shift images vertically (fraction of total height)
			horizontal_flip=False,  # randomly flip images
			vertical_flip=False)  # randomly flip images

		# Compute quantities required for feature-wise normalization
		# (std, mean, and principal components if ZCA whitening is applied).
		datagen.fit(x_train)
		# Fit the model on the batches generated by datagen.flow().
		model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard,metrics])
		model.fit_generator(datagen.flow(x_train, y_train,
			batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard, optCallback,metrics])
	else:
		print('Using real-time data augmentation.')
		# This will do preprocessing and realtime data augmentation:
		datagen = ImageDataGenerator(
			featurewise_center=False,  # set input mean to 0 over the dataset
			samplewise_center=False,  # set each sample mean to 0
			featurewise_std_normalization=False,  # divide inputs by std of the dataset
			samplewise_std_normalization=False,  # divide each input by its std
			zca_whitening=False,  # apply ZCA whitening
			rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
			width_shift_range=0,  # randomly shift images horizontally (fraction of total width)
			height_shift_range=0,  # randomly shift images vertically (fraction of total height)
			horizontal_flip=True,  # randomly flip images
			vertical_flip=True)  # randomly flip images

		# Compute quantities required for feature-wise normalization
		# (std, mean, and principal components if ZCA whitening is applied).
		datagen.fit(x_train)
		# Fit the model on the batches generated by datagen.flow().
		model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard,metrics])
		model.fit_generator(datagen.flow(x_train, y_train,
			batch_size=batch_size),
			epochs=epochs/2,
			workers=4,
			validation_data=(X_test,Y_test),
			callbacks = [tensorBoard, optCallback,metrics])

	model.save(model_path)
	print('Saved trained model at %s ' % model_path)

	# Score trained model.
	scores = model.evaluate(X_test, Y_test, verbose=1)
	print('Test loss:', scores[0])
	print('Test accuracy:', scores[1])
	print('Test precision:', statistics.mean(metrics.precisions))
	print('Test sensitivity:', statistics.mean(metrics.sensitivitys))
	print('Test specificity:', statistics.mean(metrics.specificitys))
	print('Test mcc:', statistics.mean(metrics.mccs))
	print('Test npv:', statistics.mean(metrics.npvs))
	print('Test f1:', statistics.mean(metrics.f1s))

	prec	= statistics.mean(metrics.precisions)
	sens 	= statistics.mean(metrics.sensitivitys)
	spec 	= statistics.mean(metrics.specificitys)
	mcc		= statistics.mean(metrics.mccs)
	npv		= statistics.mean(metrics.npvs)
	f1		= statistics.mean(metrics.f1s)

	f= open(resume_file,"w+")
	f.write('Name:'+ model_name+'\n\n')
	f.write('Test loss:'+ str(scores[0])+'\n')
	f.write('Test accuracy:'+ str(scores[1])+'\n')
	f.write('Test precision:'+ str(prec)+'\n')
	f.write('Test sensitivity:'+ str(sens)+'\n')
	f.write('Test specificity:'+ str(spec)+'\n')
	f.write('Test mcc:'+ str(mcc)+'\n')
	f.write('Test npv:'+ str(npv)+'\n')
	f.write('Test f1:'+ str(f1)+'\n')
	#f.write('Test mcc:', statistics.mean(metrics.val_mccs))
	f.close()

	print('Saved trained resume')
	cvscores.append([scores[0], scores[1], prec , sens, spec, mcc, npv, f1])

cvscores = nu.array(cvscores)
f= open(final_resume,"w+")
f.write('Name:'+ executionName+'\n\n')
f.write("Total loss: "+str(nu.mean(cvscores[0:len(cvscores),0]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),0]))+")\n")
f.write("Total accuracy: "+str(nu.mean(cvscores[0:len(cvscores),1]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),1]))+")\n")
f.write("Total precision: "+str(nu.mean(cvscores[0:len(cvscores),2]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),2]))+")\n")
f.write("Total sensitivity: "+str(nu.mean(cvscores[0:len(cvscores),3]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),3]))+")\n")
f.write("Total specificity: "+str(nu.mean(cvscores[0:len(cvscores),4]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),4]))+")\n")
f.write("Total mcc: "+str(nu.mean(cvscores[0:len(cvscores),5]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),5]))+")\n")
f.write("Total npv: "+str(nu.mean(cvscores[0:len(cvscores),6]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),6]))+")\n")
f.write("Total f1: "+str(nu.mean(cvscores[0:len(cvscores),7]))+" (+/- "+str(nu.std(cvscores[0:len(cvscores),7]))+")\n")
#f.write("Total mcc: %.2f%% (+/- %.2f%%)" % (nu.mean(cvscores[5]), nu.std(cvscores[5])))
f.close()

print('Program End')