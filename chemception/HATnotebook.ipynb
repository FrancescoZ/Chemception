{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescozanoli/anaconda2/envs/my-rdkit-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import input as dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 180\n",
    "MAX_NB_CHARS = 180\n",
    "EMBEDDING_DIM = 180\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Started\n",
      "Load of 24003 finished in 5.726010799407959s\n"
     ]
    }
   ],
   "source": [
    "dataComp = dataset.LoadData('data',0)\n",
    "smiles = list(map(lambda x: x._SMILE, dataComp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=C2N(Cc1ccccc1)CC(=O)C23(CC3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, char_level=True)\n",
    "tokenizer.fit_on_texts(smiles)\n",
    "print(smiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(smiles), MAX_WORD_LENGTH, MAX_WORD_LENGTH), dtype='int32')\n",
    "for i, comp in enumerate(smiles):\n",
    "    for j, char in enumerate(comp):\n",
    "        try:\n",
    "            if tokenizer.word_index[char] < MAX_NB_CHARS:\n",
    "                data[i, j, k] = tokenizer.word_index[char]\n",
    "        except:\n",
    "               None\n",
    "                    #print (char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 53 unique tokens.\n",
      "{'C': 1, 'c': 2, '(': 3, ')': 4, 'O': 5, '=': 6, '1': 7, 'N': 8, '2': 9, '3': 10, '[': 11, ']': 12, 'F': 13, '4': 14, 'l': 15, 'n': 16, 'S': 17, '@': 18, 'H': 19, '5': 20, '+': 21, '-': 22, 'B': 23, 'r': 24, '\\\\': 25, '#': 26, '6': 27, '.': 28, '/': 29, 's': 30, 'P': 31, '7': 32, 'i': 33, 'o': 34, '8': 35, 'I': 36, 'a': 37, '%': 38, '9': 39, '0': 40, 'K': 41, 'e': 42, 'A': 43, 'g': 44, 'p': 45, 'M': 46, 'T': 47, 'b': 48, 'd': 49, 'V': 50, 'Z': 51, 'G': 52, 'L': 53}\n"
     ]
    }
   ],
   "source": [
    "char_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(char_index))\n",
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (24003, 180, 180)\n",
      "Shape of label tensor: (24003, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = keras.utils.to_categorical(list(map(lambda x: 1 if x.mutagen==True else 0,dataComp)),2)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[12721.  6482.]\n",
      "[3155. 1645.]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19203 samples, validate on 4800 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(char_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_WORD_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "char_input = Input(shape=(MAX_WORD_LENGTH,), dtype='int32')\n",
    "char_sequences = embedding_layer(char_input)\n",
    "char_lstm = Bidirectional(GRU(100, return_sequences=True))(char_sequences)\n",
    "char_dense = TimeDistributed(Dense(200))(char_lstm)\n",
    "char_att = AttentionWithContext()(char_dense)\n",
    "charEncoder = Model(char_input, char_att)\n",
    "\n",
    "words_input = Input(shape=(MAX_WORD_LENGTH, MAX_WORD_LENGTH), dtype='int32')\n",
    "words_encoder = TimeDistributed(charEncoder)(words_input)\n",
    "words_lstm = Bidirectional(GRU(100, return_sequences=True))(words_encoder)\n",
    "words_dense = TimeDistributed(Dense(200))(words_lstm)\n",
    "words_att = AttentionWithContext()(words_dense)\n",
    "preds = Dense(2, activation='sigmoid')(words_att)\n",
    "model = Model(words_input, preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=1, batch_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
